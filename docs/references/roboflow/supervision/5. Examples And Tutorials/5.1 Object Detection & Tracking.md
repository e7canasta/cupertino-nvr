# Object Detection & Tracking

Relevant source files

- [docs/how_to/track_objects.md](https://github.com/roboflow/supervision/blob/1d0747fb/docs/how_to/track_objects.md)
- [examples/count_people_in_zone/README.md](https://github.com/roboflow/supervision/blob/1d0747fb/examples/count_people_in_zone/README.md)
- [examples/count_people_in_zone/inference_example.py](https://github.com/roboflow/supervision/blob/1d0747fb/examples/count_people_in_zone/inference_example.py)
- [examples/count_people_in_zone/ultralytics_example.py](https://github.com/roboflow/supervision/blob/1d0747fb/examples/count_people_in_zone/ultralytics_example.py)
- [examples/tracking/README.md](https://github.com/roboflow/supervision/blob/1d0747fb/examples/tracking/README.md)
- [examples/tracking/inference_example.py](https://github.com/roboflow/supervision/blob/1d0747fb/examples/tracking/inference_example.py)
- [examples/tracking/ultralytics_example.py](https://github.com/roboflow/supervision/blob/1d0747fb/examples/tracking/ultralytics_example.py)
- [examples/traffic_analysis/README.md](https://github.com/roboflow/supervision/blob/1d0747fb/examples/traffic_analysis/README.md)

This page covers the core functionality of Supervision for detecting and tracking objects in images and videos. The guide focuses on how to use the library to perform inference with various model types, track objects across video frames, and visualize results with annotations.

For information about zone-based counting of objects, see [Zone-based Counting](https://deepwiki.com/roboflow/supervision/5.3-zone-based-counting). For speed estimation capabilities, see [Speed Estimation](https://deepwiki.com/roboflow/supervision/5.2-speed-estimation).

## Overview

Supervision provides a streamlined workflow for object detection and tracking tasks that includes:

1. Running inference with detection models
2. Standardizing detection data with the `Detections` class
3. Tracking objects between frames with `ByteTrack`
4. Visualizing results with various annotators


```mermaid
flowchart TB
  A["Input Source<br>(Image/Video)"] --> B["Object Detection"]
  B --> C["Detections Class"]
  C --> D["Object Tracking"]
  D --> E["Visualization"]

  %% Detection methods feeding into detection
  subgraph "Detection Methods"
    B1["YOLO Models"]
    B2["Transformers"]
    B3["Segment Anything"]
    B4["Keypoint Models"]
  end
  B1 --> B
  B2 --> B
  B3 --> B
  B4 --> B

  %% Tracking pipeline
  subgraph "Tracking"
    D1["ByteTrack"]
    D2["ID Assignment"]
    D3["Motion Prediction"]
  end
  D --> D1
  D1 --> D2
  D2 --> D3

  %% Visualization components
  subgraph "Visualization Components"
    E1["BoxAnnotator"]
    E2["LabelAnnotator"]
    E3["TraceAnnotator"]
    E4["EdgeAnnotator"]
    E5["VertexAnnotator"]
  end
  E --> E1
  E --> E2
  E --> E3
  E --> E4
  E --> E5
```

Sources:

- [docs/how_to/track_objects.md8-15](https://github.com/roboflow/supervision/blob/1d0747fb/docs/how_to/track_objects.md#L8-L15) Overview of tracking capabilities.
- [examples/tracking/README.md1-8](https://github.com/roboflow/supervision/blob/1d0747fb/examples/tracking/README.md#L1-L8) Description of tracking functionality.

## Detection System

Supervision standardizes detection data through the `Detections` class, which is the central data structure for detection results. The class stores bounding boxes, confidence scores, class IDs, and other metadata for detected objects.

### Converting Model Results to Detections

Supervision supports various model frameworks and provides methods to convert their outputs to the standardized `Detections` format:


```mermaid
flowchart TB
  A["Model Output"] --> B{Model Type}

  B -->|Ultralytics YOLO| C1["Detections.from_ultralytics()"]
  B -->|Transformers| C2["Detections.from_transformers()"]
  B -->|DeepSparse| C3["Detections.from_deepsparse()"]
  B -->|Roboflow API| C4["Detections.from_inference()"]
  B -->|Detectron2| C5["Detections.from_detectron2()"]
  B -->|SAM| C6["Detections.from_sam()"]

  C1 --> D["Standardized Detections Object"]
  C2 --> D
  C3 --> D
  C4 --> D
  C5 --> D
  C6 --> D

  D --> E1["Detection Filtering"]
  D --> E2["Tracking Objects"]
  D --> E3["Visualization"]
  D --> E4["Analytics"]
```

Sources:

- [docs/how_to/track_objects.md34-39](https://github.com/roboflow/supervision/blob/1d0747fb/docs/how_to/track_objects.md#L34-L39) Information about model inference.
- [examples/tracking/inference_example.py28-32](https://github.com/roboflow/supervision/blob/1d0747fb/examples/tracking/inference_example.py#L28-L32) Example of converting model output to Detections.
- [examples/tracking/ultralytics_example.py26-30](https://github.com/roboflow/supervision/blob/1d0747fb/examples/tracking/ultralytics_example.py#L26-L30) Example with Ultralytics YOLO.

## Object Tracking with ByteTrack

Supervision implements `ByteTrack`, an algorithm for tracking objects across video frames. After detecting objects in each frame, ByteTrack associates detections across frames, assigning unique tracker IDs to each object.

### Tracking Workflow


```mermaid
flowchart LR
  A[Frame] --> B[Object Detection]
  B --> C[Detections]
  C --> D["ByteTrack.update_with_detections()"]
  D --> E[Tracked Detections]

  subgraph S[ByteTrack Algorithm]
    D1[High Confidence Matching] --> D2[Low Confidence Matching] --> D3[Track Management] --> D4[ID Assignment]
  end

  D --> D1
  E --> F[Visualization]
  E --> G[Next Frame Processing]
  G --> B
```

### Implementation Example

A typical tracking implementation contains these key components:

1. Initialize the tracker: `tracker = sv.ByteTrack()`
2. Process each frame:
    - Detect objects
    - Update tracker with new detections
    - Visualize results

Sources:

- [docs/how_to/track_objects.md98-154](https://github.com/roboflow/supervision/blob/1d0747fb/docs/how_to/track_objects.md#L98-L154) Detailed guide on tracking.
- [examples/tracking/inference_example.py20-42](https://github.com/roboflow/supervision/blob/1d0747fb/examples/tracking/inference_example.py#L20-L42) ByteTrack implementation with Inference model.
- [examples/tracking/ultralytics_example.py18-40](https://github.com/roboflow/supervision/blob/1d0747fb/examples/tracking/ultralytics_example.py#L18-L40) ByteTrack implementation with Ultralytics.

## Visualization Components

Supervision provides several annotators to visualize detections and tracking results:

|Annotator|Purpose|Primary Use Case|
|---|---|---|
|`BoxAnnotator`|Draws bounding boxes around detected objects|Object detection visualization|
|`LabelAnnotator`|Adds labels with class name and tracker ID|Identifying objects and their tracks|
|`TraceAnnotator`|Shows movement trails for tracked objects|Visualizing object motion paths|
|`EdgeAnnotator`|Draws edges between keypoints|Pose/keypoint visualization|
|`VertexAnnotator`|Draws keypoint vertices|Pose/keypoint visualization|

### Annotation Pipeline

```mermaid
flowchart LR
  A[Detections] --> B[BoxAnnotator]
  A --> C[LabelAnnotator]
  A --> G[TraceAnnotator]

  B --> D[Annotated Frame with Boxes]
  C --> E[Annotated Frame with Labels]
  D --> F[Combined Annotation]
  E --> F

  G --> H[Annotated Frame with Traces]

  F --> I[Final Annotated Frame]
  H --> I

  %% Keypoint visualization (separate subgraph)
  subgraph KPV[KeyPoint Visualization]
    J[KeyPoints] --> K[EdgeAnnotator]
    J --> L[VertexAnnotator]
    K --> M[Edges Between Joints]
    L --> N[Joint Points]
    M --> O[Pose Visualization]
    N --> O
  end
```

Sources:

- [docs/how_to/track_objects.md154-233](https://github.com/roboflow/supervision/blob/1d0747fb/docs/how_to/track_objects.md#L154-L233) Guide on visualizing tracking with annotators.
- [examples/tracking/inference_example.py21-41](https://github.com/roboflow/supervision/blob/1d0747fb/examples/tracking/inference_example.py#L21-L41) BoxAnnotator and LabelAnnotator usage.
- [docs/how_to/track_objects.md236-267](https://github.com/roboflow/supervision/blob/1d0747fb/docs/how_to/track_objects.md#L236-L267) TraceAnnotator for showing object trails.

## Working with Keypoints

Supervision also supports keypoint detection models, which can be useful for pose estimation and detailed human analysis.

### Keypoint Detection and Tracking


```mermaid
flowchart LR
  A[Frame] --> B[Keypoint Detection] --> C[KeyPoints Object]
  C --> D[KeyPoints.as_detections]
  D --> E[Detections Object] --> F[ByteTrack] --> G[Tracked Objects]

  %% Visualization branches
  C --> H1[EdgeAnnotator]
  C --> H2[VertexAnnotator]
  G --> H3[BoxAnnotator]
  G --> H4[TraceAnnotator]

  %% Final render
  H1 --> I[Annotated Frame]
  H2 --> I
  H3 --> I
  H4 --> I
```

Sources:

- [docs/how_to/track_objects.md324-404](https://github.com/roboflow/supervision/blob/1d0747fb/docs/how_to/track_objects.md#L324-L404) Guide on keypoint detection.
- [docs/how_to/track_objects.md409-484](https://github.com/roboflow/supervision/blob/1d0747fb/docs/how_to/track_objects.md#L409-L484) Converting keypoints to detections for tracking.

## Advanced Features

### Detection Smoothing

For video applications, Supervision provides the `DetectionsSmoother` to reduce jitter and improve stability of bounding boxes across frames:

```mermaid
flowchart LR
  A[Detections] --> B[tracker.update_with_detections]
  B --> C[Tracked Detections]
  C --> D[smoother.update_with_detections]

  %% Smoothing details
  subgraph S[ Smoothing Algorithm ]
    D --> D1[Temporal Filtering] --> D2[Motion Prediction] --> D3[Box Stabilization]
  end

  D --> E[Smoothed Detections] --> F[Visualization]
```

Sources:

- [docs/how_to/track_objects.md569-654](https://github.com/roboflow/supervision/blob/1d0747fb/docs/how_to/track_objects.md#L569-L654) Implementation of detection smoothing.

## Example Workflows

### Basic Object Detection and Tracking

This typical workflow shows how to:

1. Obtain a video source
2. Initialize detection model
3. Set up tracking and visualization
4. Process frames in a loop

```
1. Load video source
2. Initialize model (YOLO, Transformers, etc.)
3. Create ByteTrack tracker
4. Initialize annotators (BoxAnnotator, LabelAnnotator, etc.)
5. For each frame:
   - Perform detection
   - Convert results to Detections
   - Update tracker with detections
   - Annotate frame with results
   - Write/display annotated frame
```

Sources:

- [examples/tracking/ultralytics_example.py9-40](https://github.com/roboflow/supervision/blob/1d0747fb/examples/tracking/ultralytics_example.py#L9-L40) Complete tracking workflow with Ultralytics.
- [examples/tracking/inference_example.py10-42](https://github.com/roboflow/supervision/blob/1d0747fb/examples/tracking/inference_example.py#L10-L42) Complete tracking workflow with Inference API.
- [examples/traffic_analysis/README.md1-9](https://github.com/roboflow/supervision/blob/1d0747fb/examples/traffic_analysis/README.md#L1-L9) Real-world application example.
- [examples/count_people_in_zone/README.md1-12](https://github.com/roboflow/supervision/blob/1d0747fb/examples/count_people_in_zone/README.md#L1-L12) Application for zone-based counting.

### Zone-Based Analysis

Combine tracking with zone tools for advanced analytics:

```mermaid
flowchart TB
  A[Video Input] --> B[Object Detection]
  B --> C[Detections]
  C --> D[ByteTrack]
  D --> E[Tracked Objects]

  %% Zone logic
  E --> H[Zone.trigger]
  subgraph Z[Zone Logic]
    F[Zone Configuration] --> G[PolygonZone]
    G --> H
  end

  H --> I[Objects in Zone]
  I --> J[Visualization]
  I --> K[Analytics/Counting]
```

Sources:

- [examples/count_people_in_zone/ultralytics_example.py91-121](https://github.com/roboflow/supervision/blob/1d0747fb/examples/count_people_in_zone/ultralytics_example.py#L91-L121) Zone-based detection example.
- [examples/count_people_in_zone/inference_example.py91-125](https://github.com/roboflow/supervision/blob/1d0747fb/examples/count_people_in_zone/inference_example.py#L91-L125) Another zone example with Inference API.

## Technical Requirements

To use the object detection and tracking capabilities, you'll need:

1. A compatible model (YOLO, Transformers, etc.)
2. Video processing utilities (OpenCV typically)
3. Supervision library with optional assets extension for test videos

```
# Install with pip
pip install supervision

# For example assets
pip install supervision[assets]
```

Sources:

- [docs/how_to/track_objects.md23-27](https://github.com/roboflow/supervision/blob/1d0747fb/docs/how_to/track_objects.md#L23-L27) Instructions for downloading example videos.
- [examples/tracking/README.md26-28](https://github.com/roboflow/supervision/blob/1d0747fb/examples/tracking/README.md#L26-L28) Installation requirements.