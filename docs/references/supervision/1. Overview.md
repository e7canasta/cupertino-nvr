# Overview

Relevant source files

- [README.md](https://github.com/roboflow/supervision/blob/1d0747fb/README.md)
- [docs/changelog.md](https://github.com/roboflow/supervision/blob/1d0747fb/docs/changelog.md)
- [docs/deprecated.md](https://github.com/roboflow/supervision/blob/1d0747fb/docs/deprecated.md)
- [docs/detection/annotators.md](https://github.com/roboflow/supervision/blob/1d0747fb/docs/detection/annotators.md)
- [docs/how_to/detect_and_annotate.md](https://github.com/roboflow/supervision/blob/1d0747fb/docs/how_to/detect_and_annotate.md)
- [docs/how_to/detect_small_objects.md](https://github.com/roboflow/supervision/blob/1d0747fb/docs/how_to/detect_small_objects.md)
- [docs/index.md](https://github.com/roboflow/supervision/blob/1d0747fb/docs/index.md)
- [mkdocs.yml](https://github.com/roboflow/supervision/blob/1d0747fb/mkdocs.yml)
- [pyproject.toml](https://github.com/roboflow/supervision/blob/1d0747fb/pyproject.toml)

Supervision is a comprehensive computer vision toolkit that provides reusable utilities to streamline computer vision workflows. It offers a collection of tools for object detection, tracking, annotation, dataset management, and evaluation, designed to be model-agnostic and easily integrated with various computer vision frameworks.

This document provides a high-level overview of the Supervision library, its architecture, and core components. For installation instructions, see [Installation & Configuration](https://deepwiki.com/roboflow/supervision/1.1-installation-and-configuration), and for documentation structure, refer to [Documentation Structure](https://deepwiki.com/roboflow/supervision/1.2-documentation-structure).

Sources: [pyproject.toml1-50](https://github.com/roboflow/supervision/blob/1d0747fb/pyproject.toml#L1-L50) [README.md33-36](https://github.com/roboflow/supervision/blob/1d0747fb/README.md#L33-L36)

## Key Features

Supervision provides the following key capabilities:

- **Object Detection**: Converting outputs from various model frameworks into a standardized format
- **Visualization**: Annotating images and videos with detection results
- **Object Tracking**: Tracking objects across video frames
- **Dataset Management**: Loading, manipulating, and converting between dataset formats
- **Zone Analysis**: Counting objects in defined zones or crossing lines
- **Performance Metrics**: Evaluating model performance with metrics like mAP, precision, and recall

Supervision is designed to be compatible with Python 3.8+ and integrates with popular computer vision and deep learning frameworks.

Sources: [README.md35-36](https://github.com/roboflow/supervision/blob/1d0747fb/README.md#L35-L36) [pyproject.toml14-21](https://github.com/roboflow/supervision/blob/1d0747fb/pyproject.toml#L14-L21) [pyproject.toml36-50](https://github.com/roboflow/supervision/blob/1d0747fb/pyproject.toml#L36-L50)

## System Architecture



```mermaid
flowchart TB
  %% --- Subgraph: Utilities (colección auxiliar) ---
  subgraph Utilities
    UVideo["Video Utils"]
    UImage["Image Utils"]
    UDraw["Drawing Utils"]
    UFile["File Utils"]
    UGeom["Geometry Utils"]
  end

  %% --- Subgraph: Model Adaptors (implementaciones) ---
  subgraph "Model Adaptors"
    MAUltralytics["from_ultralytics()"]
    MATransformers["from_transformers()"]
    MAInference["from_inference()"]
    MADetectron2["from_detectron2()"]
    MADeepSparse["from_deepsparse()"]
    MASAM["from_sam()"]
    MAYOLONAS["from_yolo_nas()"]
    MALMM["from_lmm()"]
    MANCNN["from_ncnn()"]
    MAEasyOCR["from_easyocr()"]
  end

  %% --- Subgraph: Tools ---
  subgraph Tools
    ToolsHub["Tools"]
    TLineZone["LineZone"]
    TPolygonZone["PolygonZone"]
    TInferenceSlicer["InferenceSlicer"]
    TSaveDetections["SaveDetections"]
    ToolsHub --- TLineZone
    ToolsHub --- TPolygonZone
    ToolsHub --- TInferenceSlicer
    ToolsHub --- TSaveDetections
  end

  %% --- Núcleo: Supervision Library Core ---
  subgraph "Supervision Library Core"
    Detections["Detections Class"]
    Annotators["Annotators"]
    Datasets["Dataset Management"]
    Tracking["Object Tracking"]
    Metrics["Metrics & Evaluation"]
    KeyPoints["KeyPoints"]
    UtilsCore["Utilities"]
    %% no se definían edges explícitos para Datasets, KeyPoints, UtilsCore en el SVG
  end

  %% --- Nodos puente y relaciones principales del SVG ---
  Models["External ML Models"]
  MAHub["Model_Adaptors"]

  %% Edges del SVG:
  Models --> MAHub
  MAHub --> Detections
  Detections --> Annotators
  Detections --> Tracking
  Detections --> Metrics
  Detections --> ToolsHub
  Tracking --> Annotators
  ToolsHub --> Annotators
```



Supervision is organized around a central `Detections` class that standardizes object detection results from various frameworks. The library provides components for visualization, tracking, zone analysis, dataset management, and performance evaluation.

Sources: [mkdocs.yml36-93](https://github.com/roboflow/supervision/blob/1d0747fb/mkdocs.yml#L36-L93) [README.md49-66](https://github.com/roboflow/supervision/blob/1d0747fb/README.md#L49-L66) [docs/changelog.md1-200](https://github.com/roboflow/supervision/blob/1d0747fb/docs/changelog.md#L1-L200)

## Data Flow




```mermaid
flowchart LR
  A["External ML Model (YOLO, Transformers, etc.)"]
  B["Model Output"]
  C["Model Adapters: from_ultralytics(), from_transformers(), from_inference(), etc."]
  D["Detections Object"]
  E1["Visualization: BoxAnnotator, MaskAnnotator, LabelAnnotator"]
  E2["Tracking: ByteTrack"]
  E3["Zone Analysis: LineZone, PolygonZone"]
  E4["Evaluation: MeanAveragePrecision, F1Score"]
  E5["Export: SaveDetections"]

  A --> B --> C --> D
  D --> E1
  D --> E2
  D --> E3
  D --> E4
  D --> E5
  E2 --> E1
```


The data flow begins with an external model's output, which is converted into Supervision's standardized `Detections` object. This object can then be processed through various components for visualization, tracking, filtering, evaluation, and export.

Sources: [docs/how_to/detect_and_annotate.md17-46](https://github.com/roboflow/supervision/blob/1d0747fb/docs/how_to/detect_and_annotate.md#L17-L46) [README.md49-130](https://github.com/roboflow/supervision/blob/1d0747fb/README.md#L49-L130) [docs/detection/annotators.md7-26](https://github.com/roboflow/supervision/blob/1d0747fb/docs/detection/annotators.md#L7-L26)

## Core Components

### Detections System

The `Detections` class is the central data structure in Supervision, representing a set of object detections. It stores detection information such as bounding boxes, confidence scores, class IDs, and optional mask information. The class provides adapters to convert outputs from various frameworks into this standardized format.



```mermaid
classDiagram
  class Detections {
    +np.ndarray xyxy
    +np.ndarray confidence
    +np.ndarray class_id
    +np.ndarray mask
    +Dict data
    +Dict metadata
    +from_ultralytics()
    +from_transformers()
    +from_inference()
    +from_detectron2()
    +from_deepsparse()
    +from_sam()
    +from_yolo_nas()
    +from_lmm()
    +from_ncnn()
    +from_easyocr()
    +filter()
    +with_nms()
    +with_nmm()
  }
```

The Detections class provides conversion methods for various frameworks and utilities for filtering, non-maximum suppression, and other operations.

Sources: [docs/how_to/detect_and_annotate.md63-93](https://github.com/roboflow/supervision/blob/1d0747fb/docs/how_to/detect_and_annotate.md#L63-L93) [README.md54-66](https://github.com/roboflow/supervision/blob/1d0747fb/README.md#L54-L66) [docs/changelog.md569-589](https://github.com/roboflow/supervision/blob/1d0747fb/docs/changelog.md#L569-L589)

### Annotation System

Supervision provides a comprehensive set of annotators for visualizing detection results on images and videos.


```mermaid
classDiagram
  class BaseAnnotator {
    +annotate(scene, detections)
  }

  class BoxAnnotator {
    +color
    +thickness
    +annotate(scene, detections)
  }

  class MaskAnnotator {
    +color
    +opacity
    +annotate(scene, detections)
  }

  class LabelAnnotator {
    +text_color
    +text_scale
    +text_thickness
    +text_position
    +smart_position
    +annotate(scene, detections, labels)
  }

  class PolygonAnnotator
  class CircleAnnotator
  class DotAnnotator
  class TriangleAnnotator
  class EllipseAnnotator
  class TraceAnnotator
  class BlurAnnotator
  class PixelateAnnotator
  class HeatMapAnnotator
  class BackgroundOverlayAnnotator
  class IconAnnotator

  BoxAnnotator --|> BaseAnnotator
  MaskAnnotator --|> BaseAnnotator
  LabelAnnotator --|> BaseAnnotator
  PolygonAnnotator --|> BaseAnnotator
  CircleAnnotator --|> BaseAnnotator
  DotAnnotator --|> BaseAnnotator
  TriangleAnnotator --|> BaseAnnotator
  EllipseAnnotator --|> BaseAnnotator
  TraceAnnotator --|> BaseAnnotator
  BlurAnnotator --|> BaseAnnotator
  PixelateAnnotator --|> BaseAnnotator
  HeatMapAnnotator --|> BaseAnnotator
  BackgroundOverlayAnnotator --|> BaseAnnotator
  IconAnnotator --|> BaseAnnotator
```

Each annotator specializes in visualizing detections in a specific way, such as bounding boxes, masks, or labels. Annotators can be customized with parameters like color, thickness, and positioning.

Sources: [docs/detection/annotators.md6-157](https://github.com/roboflow/supervision/blob/1d0747fb/docs/detection/annotators.md#L6-L157) [README.md92-106](https://github.com/roboflow/supervision/blob/1d0747fb/README.md#L92-L106) [docs/changelog.md287-395](https://github.com/roboflow/supervision/blob/1d0747fb/docs/changelog.md#L287-L395)

### Dataset Management

Supervision provides tools for loading, manipulating, and converting between various dataset formats.



```mermaid
classDiagram
  class DetectionDataset {
    +classes: List[str]
    +from_coco()
    +from_yolo()
    +from_pascal_voc()
    +as_coco()
    +as_yolo()
    +as_pascal_voc()
    +split()
    +merge()
  }

  class ClassificationDataset {
    +classes: List[str]
    +split()
    +merge()
  }
```


The dataset management system supports common formats like COCO, YOLO, and Pascal VOC, and provides methods for splitting, merging, and converting datasets.

Sources: [README.md110-220](https://github.com/roboflow/supervision/blob/1d0747fb/README.md#L110-L220) [docs/changelog.md419-452](https://github.com/roboflow/supervision/blob/1d0747fb/docs/changelog.md#L419-L452)

### Tracking System

Supervision includes object tracking capabilities based on ByteTrack:



```mermaid
classDiagram
  class ByteTrack {
    +reset()
    +update_with_detections()
  }

  class STrack {
    +track_id
    +xyxy
    +confidence
    +class_id
  }

  ByteTrack ..> STrack : creates
```

The tracking system can assign consistent IDs to objects across video frames, enabling temporal analysis of object movements.

Sources: [docs/changelog.md9-35](https://github.com/roboflow/supervision/blob/1d0747fb/docs/changelog.md#L9-L35) [docs/changelog.md140-145](https://github.com/roboflow/supervision/blob/1d0747fb/docs/changelog.md#L140-L145) [docs/deprecated.md33](https://github.com/roboflow/supervision/blob/1d0747fb/docs/deprecated.md#L33-L33)

### Zone Analysis Tools

Supervision offers zone analysis tools for counting objects in specific areas or crossing defined lines:



```mermaid
flowchart TB
  subgraph "Zone Tools"
    D[Detections]

    L[LineZone<br/>- start: Point<br/>- end: Point<br/>- trigger_count detections]
    P[PolygonZone<br/>- polygon: np.ndarray<br/>- triggering_anchors<br/>- trigger detections]

    LA[LineZoneAnnotator]
    LAM[LineZoneAnnotatorMulticlass]
    PA[PolygonZoneAnnotator]

    D --> L
    D --> P
    L --> LA
    L --> LAM
    P --> PA
  end
```

These tools enable applications like traffic counting, crowd analysis, and zone-based monitoring.

Sources: [docs/changelog.md7-9](https://github.com/roboflow/supervision/blob/1d0747fb/docs/changelog.md#L7-L9) [docs/changelog.md170-213](https://github.com/roboflow/supervision/blob/1d0747fb/docs/changelog.md#L170-L213) [docs/how_to/detect_small_objects.md150-156](https://github.com/roboflow/supervision/blob/1d0747fb/docs/how_to/detect_small_objects.md#L150-L156)

### Inference Tools

Supervision provides tools to enhance inference capabilities, particularly for challenging scenarios:


```mermaid
flowchart TB
  subgraph "Inference Tools"
    I[Large Image]
    S[InferenceSlicer<br/>- callback: Callable<br/>- slice_wh<br/>- overlap_wh<br/>- overlap_strategy]
    IS[Image Slices]
    SD[Slice Detections]
    D[Combined Detections]

    I --> S
    S -->|1. Slice image| IS
    IS -->|2. Run inference on each slice| SD
    SD -->|3. Merge results| D
  end
```


The `InferenceSlicer` tool is particularly useful for detecting small objects in high-resolution images by slicing the image into smaller segments for processing.

Sources: [docs/how_to/detect_small_objects.md149-255](https://github.com/roboflow/supervision/blob/1d0747fb/docs/how_to/detect_small_objects.md#L149-L255) [docs/changelog.md433-437](https://github.com/roboflow/supervision/blob/1d0747fb/docs/changelog.md#L433-L437) [docs/deprecated.md16-18](https://github.com/roboflow/supervision/blob/1d0747fb/docs/deprecated.md#L16-L18)

## Framework Integration

Supervision is designed to integrate seamlessly with various computer vision and deep learning frameworks:



```mermaid
flowchart TB
  %% Lanes
  subgraph "External Frameworks"
    UL[Ultralytics YOLO]
    HF[Hugging Face Transformers]
    RF[Roboflow Inference]
    D2[Detectron2]
    DS[DeepSparse]
    SAM[Segment Anything Model]
    YNAS[YOLO-NAS]
    NC[NCNN]
    OCR[EasyOCR]
    LMM[Large Multimodal Models]
  end

  subgraph "Supervision Adapters"
    AUL[from_ultralytics]
    AHF[from_transformers]
    ARF[from_inference]
    AD2[from_detectron2]
    ADS[from_deepsparse]
    ASAM[from_sam]
    AYNAS[from_yolo_nas]
    ANC[from_ncnn]
    AOCR[from_easyocr]
    ALMM[from_lmm]
  end

  subgraph "Supervision Core"
    DET[Detections]
    KP[KeyPoints]
  end

  %% External -> Adapters
  UL --> AUL
  HF --> AHF
  RF --> ARF
  D2 --> AD2
  DS --> ADS
  SAM --> ASAM
  YNAS --> AYNAS
  NC --> ANC
  OCR --> AOCR
  LMM --> ALMM

  %% Adapters -> Core
  AUL --> DET
  AHF --> DET
  ARF --> DET
  AD2 --> DET
  ADS --> DET
  ASAM --> DET
  AYNAS --> DET
  ANC --> DET
  AOCR --> DET
  ALMM --> DET

  AUL --> KP
  AHF --> KP
  ARF --> KP
  AD2 --> KP
  AYNAS --> KP

```

This architecture allows Supervision to work with outputs from virtually any computer vision model by providing standardized adapters for popular frameworks.

Sources: [docs/how_to/detect_and_annotate.md122-129](https://github.com/roboflow/supervision/blob/1d0747fb/docs/how_to/detect_and_annotate.md#L122-L129) [docs/changelog.md216-270](https://github.com/roboflow/supervision/blob/1d0747fb/docs/changelog.md#L216-L270) [docs/changelog.md452-461](https://github.com/roboflow/supervision/blob/1d0747fb/docs/changelog.md#L452-L461)

## Usage Patterns

The typical usage pattern for Supervision involves:

1. **Model Inference**: Run your model on the input data (image/video)
2. **Convert to Detections**: Convert the model output to the Supervision `Detections` format
3. **Process Detections**: Apply tracking, filtering, or zone analysis as needed
4. **Visualize Results**: Use annotators to visualize the results on the original image/video



```mermaid
sequenceDiagram
    participant Model as "CV Model"
    participant Supervision as "Supervision"
    participant Application as "Your Application"

    Application->>Model: Run inference on image/video
    Model-->>Application: Return model-specific results

    Application->>Supervision: Convert to Detections
    Note over Supervision: Detections.from_ultralytics()\nDetections.from_transformers()\netc.

    opt Object Tracking
        Application->>Supervision: Track objects
        Note over Supervision: tracker.update_with_detections()
    end

    opt Zone Analysis
        Application->>Supervision: Analyze zones
        Note over Supervision: line_zone.trigger()\npolygon_zone.trigger()
    end

    Application->>Supervision: Annotate results
    Note over Supervision: box_annotator.annotate()\nmask_annotator.annotate()\nlabel_annotator.annotate()

    Supervision-->>Application: Return annotated image/video
```


This modular approach allows users to flexibly combine various components of Supervision based on their specific needs.

Sources: [docs/how_to/detect_and_annotate.md130-172](https://github.com/roboflow/supervision/blob/1d0747fb/docs/how_to/detect_and_annotate.md#L130-L172) [README.md54-106](https://github.com/roboflow/supervision/blob/1d0747fb/README.md#L54-L106) [docs/detection/annotators.md10-26](https://github.com/roboflow/supervision/blob/1d0747fb/docs/detection/annotators.md#L10-L26)

## Example Applications

Supervision enables a wide range of computer vision applications:

|Application Type|Relevant Components|Example Use Case|
|---|---|---|
|Object Detection|`Detections`, Box/Mask Annotators|Identifying objects in images|
|Instance Segmentation|`Detections` with masks, `MaskAnnotator`|Pixel-precise object delineation|
|Object Tracking|`ByteTrack`, `TraceAnnotator`|Following objects across video frames|
|Traffic Analysis|`LineZone`, `PolygonZone`|Counting vehicles crossing a line|
|Small Object Detection|`InferenceSlicer`|Detecting small objects in aerial imagery|
|Dataset Processing|`DetectionDataset`|Converting between dataset formats|
|Performance Evaluation|`MeanAveragePrecision`, `F1Score`|Evaluating model accuracy|

Supervision's modular design makes it suitable for both simple visualization tasks and complex computer vision pipelines.

Sources: [README.md223-241](https://github.com/roboflow/supervision/blob/1d0747fb/README.md#L223-L241) [docs/changelog.md149-165](https://github.com/roboflow/supervision/blob/1d0747fb/docs/changelog.md#L149-L165) [docs/how_to/detect_small_objects.md5-16](https://github.com/roboflow/supervision/blob/1d0747fb/docs/how_to/detect_small_objects.md#L5-L16)

## Summary

Supervision is a comprehensive toolkit for computer vision tasks, providing:

1. A standardized representation of detection results (`Detections` class)
2. Rich visualization capabilities (various annotators)
3. Object tracking functionality (`ByteTrack`)
4. Dataset management tools
5. Zone analysis utilities
6. Performance metrics
7. Adapters for various ML frameworks

Its flexibility and interoperability with multiple frameworks make it a valuable tool for computer vision practitioners, allowing them to focus on application logic rather than boilerplate code.

Sources: [README.md33-46](https://github.com/roboflow/supervision/blob/1d0747fb/README.md#L33-L46) [pyproject.toml2-3](https://github.com/roboflow/supervision/blob/1d0747fb/pyproject.toml#L2-L3) [mkdocs.yml1-4](https://github.com/roboflow/supervision/blob/1d0747fb/mkdocs.yml#L1-L4)